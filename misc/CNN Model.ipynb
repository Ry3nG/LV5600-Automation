{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Constants import Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Global Variables\n",
    "most of the changes are done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "data_dir = Constants.PATH_RDI_DATASET_PREPROCESSED\n",
    "\n",
    "# Define categories\n",
    "categories = Constants.TEST_CATEGORIES\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.25  # Remaining 5% is for test\n",
    "\n",
    "# Model path\n",
    "MODEL_PATH = Constants.MODEL_PATH\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT, IMG_WIDTH = 240, 670\n",
    "\n",
    "# Define batch size and epochs\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"{directory} exists.\")\n",
    "    else:\n",
    "        print(f\"{directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_dir, categories, train_ratio, val_ratio):\n",
    "    # Check directory\n",
    "    check_directory(data_dir)\n",
    "    \n",
    "    # Define directories\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    total_train, total_val, total_test = 0, 0, 0\n",
    "\n",
    "    # Loop through each category and split the data\n",
    "    for cat in categories:\n",
    "        cat_dir = os.path.join(data_dir, cat)\n",
    "        img_files = os.listdir(cat_dir)\n",
    "        random.shuffle(img_files)  # shuffle the list of image files\n",
    "\n",
    "        train_files = img_files[:int(len(img_files) * train_ratio)]\n",
    "        val_files = img_files[int(len(img_files) * train_ratio):int(len(img_files) * (train_ratio + val_ratio))]\n",
    "        test_files = img_files[int(len(img_files) * (train_ratio + val_ratio)):]\n",
    "\n",
    "        for d in [(train_files, train_dir), (val_files, val_dir), (test_files, test_dir)]:\n",
    "            os.makedirs(os.path.join(d[1], cat), exist_ok=True)\n",
    "            for fname in d[0]:\n",
    "                shutil.copy(os.path.join(cat_dir, fname), os.path.join(d[1], cat, fname))\n",
    "\n",
    "        total_train += len(train_files)\n",
    "        total_val += len(val_files)\n",
    "        total_test += len(test_files)\n",
    "\n",
    "    return train_dir, val_dir, test_dir, total_train, total_val, total_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generators(train_dir, val_dir, test_dir):\n",
    "    # Preprocess the images\n",
    "    # Introduce data augmentation to the training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, \n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2, \n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    # We do not apply any augmentation on validation data\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.2),  \n",
    "        Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.3),  \n",
    "        Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.4),  \n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # added L2 regularization, originally 0.001\n",
    "        Dropout(0.5),  \n",
    "        Dense(3)  # number of classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, val_generator, total_train, total_val):\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # Monitor validation loss\n",
    "        patience=2,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best value\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=total_train // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=total_val // BATCH_SIZE,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(history):\n",
    "    # Plotting the training progress\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, history, total_train, total_val):\n",
    "    # Save the model and rename according to the number of epochs and timestamp\n",
    "    now = datetime.datetime.now()\n",
    "    model_name = 'model_' + str(EPOCHS) + '_' + now.strftime(\"%Y%m%d-%H%M%S\") + '.h5'\n",
    "    model.save(MODEL_PATH + model_name)\n",
    "\n",
    "    # Generate a txt file with detailed information about the model\n",
    "    with open(MODEL_PATH + model_name + '.txt', 'w') as f:\n",
    "        f.write('Model name: ' + model_name + '\\n')\n",
    "        f.write('Number of epochs: ' + str(EPOCHS) + '\\n')\n",
    "        f.write('Batch size: ' + str(BATCH_SIZE) + '\\n')\n",
    "        f.write('Image height: ' + str(IMG_HEIGHT) + '\\n')\n",
    "        f.write('Image width: ' + str(IMG_WIDTH) + '\\n')\n",
    "        f.write('Total training images: ' + str(total_train) + '\\n')\n",
    "        f.write('Total validation images: ' + str(total_val) + '\\n')\n",
    "        f.write('Training accuracy: ' + str(history.history['accuracy'][-1]) + '\\n')\n",
    "        f.write('Validation accuracy: ' + str(history.history['val_accuracy'][-1]) + '\\n')\n",
    "        f.write('Training loss: ' + str(history.history['loss'][-1]) + '\\n')\n",
    "        f.write('Validation loss: ' + str(history.history['val_loss'][-1]) + '\\n')\n",
    "        f.write('Timestamp: ' + now.strftime(\"%Y%m%d-%H%M%S\") + '\\n')\n",
    "        f.write('Model summary: ' + '\\n')\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))  # save model summary to txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Main Functions\n",
    "Do not change anything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\M15\\Leader LV5600 PoC\\Dataset\\RDI Dataset\\Preprocessed\\ exists.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# data generator\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dir, val_dir, test_dir, total_train, total_val, total_test \u001b[39m=\u001b[39m split_data(\n\u001b[0;32m      3\u001b[0m     data_dir, categories, train_ratio, val_ratio\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m train_generator, val_generator, test_generator \u001b[39m=\u001b[39m get_data_generators(\n\u001b[0;32m      6\u001b[0m     train_dir, val_dir, test_dir\n\u001b[0;32m      7\u001b[0m )\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36msplit_data\u001b[1;34m(data_dir, categories, train_ratio, val_ratio)\u001b[0m\n\u001b[0;32m     28\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(d[\u001b[39m1\u001b[39m], cat), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m     \u001b[39mfor\u001b[39;00m fname \u001b[39min\u001b[39;00m d[\u001b[39m0\u001b[39m]:\n\u001b[1;32m---> 30\u001b[0m         shutil\u001b[39m.\u001b[39;49mcopy(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(cat_dir, fname), os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(d[\u001b[39m1\u001b[39;49m], cat, fname))\n\u001b[0;32m     32\u001b[0m total_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_files)\n\u001b[0;32m     33\u001b[0m total_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(val_files)\n",
      "File \u001b[1;32mc:\\Users\\TAS_Backup_PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:419\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dst):\n\u001b[0;32m    418\u001b[0m     dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(src))\n\u001b[1;32m--> 419\u001b[0m copyfile(src, dst, follow_symlinks\u001b[39m=\u001b[39;49mfollow_symlinks)\n\u001b[0;32m    420\u001b[0m copymode(src, dst, follow_symlinks\u001b[39m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    421\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[1;32mc:\\Users\\TAS_Backup_PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:256\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    254\u001b[0m     os\u001b[39m.\u001b[39msymlink(os\u001b[39m.\u001b[39mreadlink(src), dst)\n\u001b[0;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(src, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fsrc:\n\u001b[0;32m    257\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(dst, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[0;32m    259\u001b[0m                 \u001b[39m# macOS\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data generator\n",
    "train_dir, val_dir, test_dir, total_train, total_val, total_test = split_data(\n",
    "    data_dir, categories, train_ratio, val_ratio\n",
    ")\n",
    "train_generator, val_generator, test_generator = get_data_generators(\n",
    "    train_dir, val_dir, test_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = get_model()\n",
    "history = train_model(model, train_generator, val_generator, total_train, total_val)\n",
    "plot_training_progress(history)\n",
    "save_model(model, history, total_train, total_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
