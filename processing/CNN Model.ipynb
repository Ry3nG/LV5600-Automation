{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TAS_Backup_PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\__init__.py:148\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m    146\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    149\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    150\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1149\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Constants import Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Global Variables\n",
    "most of the changes are done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "data_dir = Constants.PATH_RDI_DATASET_RAW\n",
    "\n",
    "# Define categories\n",
    "categories = Constants.TEST_CATEGORIES\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.15  # Remaining 15% is for test\n",
    "\n",
    "# Model path\n",
    "MODEL_PATH = Constants.MODEL_PATH\n",
    "\n",
    "# Image dimensions\n",
    "IMG_HEIGHT, IMG_WIDTH = 240, 670\n",
    "\n",
    "# Define batch size and epochs\n",
    "BATCH_SIZE = 32 \n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"{directory} exists.\")\n",
    "    else:\n",
    "        print(f\"{directory} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_dir, categories, train_ratio, val_ratio):\n",
    "    # Check directory\n",
    "    check_directory(data_dir)\n",
    "    \n",
    "    # Define directories\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    total_train, total_val, total_test = 0, 0, 0\n",
    "\n",
    "    # Loop through each category and split the data\n",
    "    for cat in categories:\n",
    "        cat_dir = os.path.join(data_dir, cat)\n",
    "        img_files = os.listdir(cat_dir)\n",
    "        random.shuffle(img_files)  # shuffle the list of image files\n",
    "\n",
    "        train_files = img_files[:int(len(img_files) * train_ratio)]\n",
    "        val_files = img_files[int(len(img_files) * train_ratio):int(len(img_files) * (train_ratio + val_ratio))]\n",
    "        test_files = img_files[int(len(img_files) * (train_ratio + val_ratio)):]\n",
    "\n",
    "        for d in [(train_files, train_dir), (val_files, val_dir), (test_files, test_dir)]:\n",
    "            os.makedirs(os.path.join(d[1], cat), exist_ok=True)\n",
    "            for fname in d[0]:\n",
    "                shutil.copy(os.path.join(cat_dir, fname), os.path.join(d[1], cat, fname))\n",
    "\n",
    "        total_train += len(train_files)\n",
    "        total_val += len(val_files)\n",
    "        total_test += len(test_files)\n",
    "\n",
    "    return train_dir, val_dir, test_dir, total_train, total_val, total_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_generators(train_dir, val_dir, test_dir):\n",
    "    # Preprocess the images\n",
    "    # Introduce data augmentation to the training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, \n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2, \n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "    # We do not apply any augmentation on validation data\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.2),  \n",
    "        Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.3),  \n",
    "        Conv2D(128, 3, padding='same', activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Dropout(0.4),  \n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # added L2 regularization, originally 0.001\n",
    "        Dropout(0.5),  \n",
    "        Dense(3)  # number of classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, val_generator, total_train, total_val):\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # Monitor validation loss\n",
    "        patience=2,  # Number of epochs with no improvement after which training will be stopped\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best value\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=total_train // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=total_val // BATCH_SIZE,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_progress(history):\n",
    "    # Plotting the training progress\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, history, total_train, total_val):\n",
    "    # Save the model and rename according to the number of epochs and timestamp\n",
    "    now = datetime.datetime.now()\n",
    "    model_name = 'model_' + str(EPOCHS) + '_' + now.strftime(\"%Y%m%d-%H%M%S\") + '.h5'\n",
    "    model.save(MODEL_PATH + model_name)\n",
    "\n",
    "    # Generate a txt file with detailed information about the model\n",
    "    with open(MODEL_PATH + model_name + '.txt', 'w') as f:\n",
    "        f.write('Model name: ' + model_name + '\\n')\n",
    "        f.write('Number of epochs: ' + str(EPOCHS) + '\\n')\n",
    "        f.write('Batch size: ' + str(BATCH_SIZE) + '\\n')\n",
    "        f.write('Image height: ' + str(IMG_HEIGHT) + '\\n')\n",
    "        f.write('Image width: ' + str(IMG_WIDTH) + '\\n')\n",
    "        f.write('Total training images: ' + str(total_train) + '\\n')\n",
    "        f.write('Total validation images: ' + str(total_val) + '\\n')\n",
    "        f.write('Training accuracy: ' + str(history.history['accuracy'][-1]) + '\\n')\n",
    "        f.write('Validation accuracy: ' + str(history.history['val_accuracy'][-1]) + '\\n')\n",
    "        f.write('Training loss: ' + str(history.history['loss'][-1]) + '\\n')\n",
    "        f.write('Validation loss: ' + str(history.history['val_loss'][-1]) + '\\n')\n",
    "        f.write('Timestamp: ' + now.strftime(\"%Y%m%d-%H%M%S\") + '\\n')\n",
    "        f.write('Model summary: ' + '\\n')\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))  # save model summary to txt file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Main Functions\n",
    "Do not change anything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\M15_PoC\\Dataset\\RDI Dataset\\PreProcessed\\ exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11546 images belonging to 3 classes.\n",
      "Found 2474 images belonging to 3 classes.\n",
      "Found 2475 images belonging to 3 classes.\n",
      "Epoch 1/20\n",
      "360/360 [==============================] - 1072s 3s/step - loss: 0.9018 - accuracy: 0.8322 - val_loss: 0.4276 - val_accuracy: 0.9261\n",
      "Epoch 2/20\n",
      "360/360 [==============================] - 1049s 3s/step - loss: 0.5790 - accuracy: 0.8696 - val_loss: 0.3574 - val_accuracy: 0.9517\n",
      "Epoch 3/20\n",
      "133/360 [==========>...................] - ETA: 10:36 - loss: 0.5115 - accuracy: 0.8863"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_dir, val_dir, test_dir, total_train, total_val, total_test = split_data(\n",
    "        data_dir, categories, train_ratio, val_ratio)\n",
    "    train_generator, val_generator, test_generator = get_data_generators(\n",
    "        train_dir, val_dir, test_dir)\n",
    "    model = get_model()\n",
    "    history = train_model(model, train_generator, val_generator, total_train, total_val)\n",
    "    plot_training_progress(history)\n",
    "    save_model(model, history, total_train, total_val)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
